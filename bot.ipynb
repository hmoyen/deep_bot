{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb2cb16",
   "metadata": {},
   "source": [
    "# **Step 1: Install Dependencies**\n",
    "Before running the code, ensure you have the necessary packages installed.\n",
    "\n",
    "## Install Ollama\n",
    "Ollama is required for embeddings and chat-based interactions.\n",
    "\n",
    "```bash\n",
    "# Install Ollama\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Pull the DeepSeek-R1 model\n",
    "ollama pull deepseek-r1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec53a0a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Load the PDF and Create Embeddings**\n",
    "\n",
    "# **Step 2: Load and Process the PDF**\n",
    "We will:\n",
    "1. Load a PDF using `PyMuPDFLoader`.\n",
    "2. Split it into smaller text chunks for efficient retrieval.\n",
    "3. Generate embeddings using `OllamaEmbeddings` with DeepSeek-R1.\n",
    "4. Store these embeddings in a **ChromaDB** vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ef813",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio langchain langchain-community chromadb pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9657b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gradio as gr\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "\n",
    "# Load the document\n",
    "loader = PyMuPDFLoader(\"document-20-24.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize embeddings model\n",
    "embedding_function = OllamaEmbeddings(model=\"deepseek-r1\")\n",
    "\n",
    "# Generate embeddings in parallel\n",
    "def generate_embedding(chunk):\n",
    "    return embedding_function.embed_query(chunk.page_content)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    embeddings = list(executor.map(generate_embedding, chunks))\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = Client(Settings())\n",
    "\n",
    "# Delete and create new collection\n",
    "try:\n",
    "    client.delete_collection(name=\"foundations_of_llms\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error deleting collection: {e}\")\n",
    "\n",
    "collection = client.create_collection(name=\"foundations_of_llms\")\n",
    "\n",
    "# Add documents and embeddings to Chroma\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk.page_content], \n",
    "        metadatas=[{'id': idx}], \n",
    "        embeddings=[embeddings[idx]], \n",
    "        ids=[str(idx)]\n",
    "    )\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = Chroma(collection_name=\"foundations_of_llms\", client=client, embedding_function=embedding_function).as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b85cc",
   "metadata": {},
   "source": [
    "# **Step 3: Context Retrieval and Chat**\n",
    "Now, we will:\n",
    "1. **Retrieve** relevant sections from ChromaDB based on user queries.\n",
    "2. **Generate responses** using `Ollama` with DeepSeek-R1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e080eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11503/3410436889.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"deepseek-r1\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM for answering questions\n",
    "llm = Ollama(model=\"deepseek-r1\")\n",
    "\n",
    "def retrieve_context(question):\n",
    "    \"\"\"Retrieve relevant context from stored embeddings.\"\"\"\n",
    "    results = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    return context\n",
    "\n",
    "def query_deepseek(question, context):\n",
    "    \"\"\"Use DeepSeek-R1 to generate an answer.\"\"\"\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # Clean response\n",
    "    final_answer = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "    return final_answer\n",
    "\n",
    "def ask_question(question):\n",
    "    \"\"\"Retrieve context and answer the question.\"\"\"\n",
    "    context = retrieve_context(question)\n",
    "    answer = query_deepseek(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b498303",
   "metadata": {},
   "source": [
    "# **Step 4: Deploy the Chatbot UI**\n",
    "Finally, we will use **Gradio** to create a simple web interface where users can input their questions and receive answers based on the processed PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e59dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Volvo Chatbot: Troubleshooting\",\n",
    "    description=\"Ask any question about the Volvo manual book. Powered by DeepSeek-R1.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df6282",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "\n",
    "    üìÑ This notebook will process the PDF once and store embeddings in ChromaDB, so you don‚Äôt need to reprocess it every time.\n",
    "    ‚ö° The chatbot retrieves only relevant sections before generating an answer using DeepSeek-R1.\n",
    "    üöÄ Gradio provides an easy-to-use UI for interacting with the chatbot.\n",
    "\n",
    "Try running each cell one by one in a Jupyter Notebook, and let me know if you need any modifications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
